# -*- coding: utf-8 -*-
"""dl 1 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1npjVY1zh65KELgIyrOQBunbagSitpjby
"""

import os
import zipfile
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense,
                                     Lambda, Dropout, BatchNormalization, LeakyReLU)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import random

def extract_zip(zip_path, extract_to="dataset"):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    return extract_to

datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)

def load_images(data_path, target_size=(100, 100)):
    images = {}
    if not os.path.exists(data_path) or not os.listdir(data_path):
        print("Error: Dataset folder is empty or missing!")
        return images

    for file in os.listdir(data_path):
        if file.endswith(".bmp"):
            parts = file.split("_")
            if len(parts) < 2:
                continue
            user_id = parts[0]
            img_path = os.path.join(data_path, file)
            try:
                img = load_img(img_path, target_size=target_size, color_mode='grayscale')
                img = img_to_array(img) / 255.0
                img = datagen.random_transform(img)  # Apply augmentation
                if user_id not in images:
                    images[user_id] = []
                images[user_id].append(img)
            except Exception as e:
                print(f"Error loading {img_path}: {e}")
    return images

def visualize_images(images, num_samples=5):
    plt.figure(figsize=(10, 5))
    users = random.sample(list(images.keys()), min(num_samples, len(images)))
    for i, user in enumerate(users):
        plt.subplot(1, num_samples, i + 1)
        plt.imshow(images[user][0].squeeze(), cmap='gray')
        plt.title(f"User {user}")
        plt.axis('off')
    plt.show()

def create_pairs(images):
    pairs, labels = [], []
    user_ids = list(images.keys())

    if len(user_ids) < 2:
        print("Not enough fingerprint IDs to create pairs!")
        return np.array([]), np.array([])

    for user in user_ids:
        impressions = images[user]
        if len(impressions) < 2:
            print(f"Skipping {user}, not enough impressions.")
            continue

        for i in range(len(impressions) - 1):
            for j in range(i + 1, len(impressions)):
                pairs.append([impressions[i], impressions[j]])
                labels.append(1)

        other_users = [u for u in user_ids if u != user]
        if other_users:
            num_impostor_pairs = max(len(impressions), 10)  # Ensure impostor pairs are balanced
            for _ in range(num_impostor_pairs):
                other_user = random.choice(other_users)
                if images[other_user]:
                    impostor_img = random.choice(images[other_user])
                    pairs.append([impressions[0], impostor_img])
                    labels.append(0)

    print(f"Total pairs generated: {len(pairs)}")
    return np.array(pairs), np.array(labels)

def evaluate_model(siamese_model, X_test, y_test):
    predictions = siamese_model.predict([X_test[:, 0], X_test[:, 1]])
    predicted_labels = (predictions > 0.5).astype(int).flatten()
    accuracy = accuracy_score(y_test, predicted_labels)
    precision = precision_score(y_test, predicted_labels)
    recall = recall_score(y_test, predicted_labels)
    f1 = f1_score(y_test, predicted_labels)
    print(f"Best Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

def build_siamese_network(input_shape):
    input_layer = Input(shape=input_shape)
    x = Conv2D(64, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001))(input_layer)
    x = LeakyReLU()(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D()(x)
    x = Conv2D(128, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
    x = LeakyReLU()(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D()(x)
    x = Conv2D(256, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
    x = LeakyReLU()(x)
    x = BatchNormalization()(x)
    x = Flatten()(x)
    x = Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
    x = LeakyReLU()(x)
    x = Dropout(0.5)(x)
    embedding_model = Model(input_layer, x)

    input_a = Input(shape=input_shape)
    input_b = Input(shape=input_shape)
    encoded_a = embedding_model(input_a)
    encoded_b = embedding_model(input_b)

    distance = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([encoded_a, encoded_b])
    output = Dense(1, activation='sigmoid')(distance)

    siamese_model = Model(inputs=[input_a, input_b], outputs=output)
    siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00005), metrics=['accuracy'])
    return siamese_model

dataset_path = extract_zip("fingerprint_bitmaps.zip")
data_folder = dataset_path
images = load_images(data_folder)
visualize_images(images)

pairs, labels = create_pairs(images)
if len(pairs) == 0:
    raise ValueError("No valid pairs generated. Check dataset!")

X_train, X_test, y_train, y_test = train_test_split(pairs, labels, test_size=0.2, random_state=42)

input_shape = X_train[0][0].shape
siamese_model = build_siamese_network(input_shape)

early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

siamese_model.fit([X_train[:, 0], X_train[:, 1]], y_train,
                  validation_data=([X_test[:, 0], X_test[:, 1]], y_test),
                  epochs=30, batch_size=16, callbacks=[early_stopping])

evaluate_model(siamese_model, X_test, y_test)

def visualize_authentication(test_img, ref_imgs, similarities):
    plt.figure(figsize=(10, 5))
    plt.subplot(1, len(ref_imgs) + 1, 1)
    plt.imshow(test_img.squeeze(), cmap='gray')
    plt.title("Test Image")
    plt.axis('off')

    for i, (ref_img, similarity) in enumerate(zip(ref_imgs, similarities)):
        plt.subplot(1, len(ref_imgs) + 1, i + 2)
        plt.imshow(ref_img.squeeze(), cmap='gray')
        plt.title(f"Sim: {similarity:.2f}")
        plt.axis('off')

    plt.show()

def authenticate_user(user_id, test_image_path, reference_images, threshold=0.5):
    if user_id not in reference_images:
        print("User ID not found in the database!")
        return False

    test_img = load_img(test_image_path, target_size=(100, 100), color_mode='grayscale')
    test_img = img_to_array(test_img) / 255.0
    test_img = np.expand_dims(test_img, axis=0)

    similarities = []
    ref_imgs = []
    for ref_img in reference_images[user_id]:
        ref_imgs.append(ref_img)
        ref_img = np.expand_dims(ref_img, axis=0)
        prob = siamese_model.predict([test_img, ref_img])[0][0]
        similarities.append(prob)

    avg_similarity = np.mean(similarities)
    print(f"Authentication confidence for User {user_id}: {avg_similarity:.4f}")

    visualize_authentication(test_img, ref_imgs[:5], similarities[:5])

    return avg_similarity > threshold

sample_user = random.choice(list(images.keys()))
test_image_path = os.path.join(data_folder, random.choice(os.listdir(data_folder)))
authenticate_user(sample_user, test_image_path, images)

