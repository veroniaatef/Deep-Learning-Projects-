# -*- coding: utf-8 -*-
"""20216054-20217012-20227040-20216079-s2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pxCZiBSkRFx-nQXoshxG60wFdmhgR0C0

Doha Sami 20216054
Meral Mostafa 20217012
Nourhan Salah 20227040
Veronia Atef 20216079
"""

!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip
!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip
!unzip -qq Flickr8k_Dataset.zip
!unzip -qq Flickr8k_text.zip
!rm Flickr8k_Dataset.zip Flickr8k_text.zip

import os
import string
from collections import defaultdict
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess
from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess
import numpy as np
import random
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.applications import EfficientNetB7, ResNet152
from tensorflow.keras.layers import GlobalAveragePooling2D
from tqdm import tqdm
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Input, Dense, GRU, Embedding, Dropout, add
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import GRU, BatchNormalization, Concatenate, LayerNormalization
from tensorflow.keras.regularizers import l2

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

#Ensures truncated/corrupt images can still be loaded.

def load_captions(filename):
    captions = {}
    with open(filename, 'r') as file:
        for line in file:
            tokens = line.strip().split('\t')
            if len(tokens) == 2:
                image_id, caption = tokens
                image_id = image_id.split('#')[0]
                captions.setdefault(image_id, []).append(caption)
    return captions
captions = load_captions('Flickr8k.token.txt')
print('Total images with captions:', len(captions))

print("Sample captions:")
for img, caps in list(captions.items())[:3]:
    print(f"\nImage ID: {img}")
    for cap in caps:
        print(f"- {cap}")

def preprocess_image(image_path, model_type='efficientnet'):
    try:
        if model_type == 'efficientnet':
            target_size = (600, 600)
            preprocess_fn = efficientnet_preprocess
        elif model_type == 'resnet':
            target_size = (448, 448)
            preprocess_fn = resnet_preprocess
        else:
            target_size = (224, 224)
            preprocess_fn = vgg16_preprocess

        image = load_img(image_path, target_size=target_size)
        image = img_to_array(image)
        image = np.expand_dims(image, axis=0)
        # Converts to array and adds batch dimension (1, H, W, C)

        image = preprocess_fn(image)
        return image

     #Applies normalization for the specific pre-trained model

    except Exception as e:
        print(f"Error loading image {image_path}: {e}")
        return None

def clean_captions(captions_dict):
    table = str.maketrans('', '', string.punctuation)
    cleaned = {}
    for img, caps in captions_dict.items():
        cleaned_caps = []
        for cap in caps:
            cap = cap.lower()
            cap = cap.translate(table)
            cap = cap.split()
            cap = [word for word in cap if len(word) > 1 and word.isalpha()]
            cleaned_caps.append(' '.join(cap))
        cleaned[img] = cleaned_caps
    return cleaned

#Lowercases text.

#Removes punctuation.

#Removes extra spaces.

#Adds startseq and endseq tokens to each caption â€” useful for training the model to predict the sequence of words.

captions_cleaned = clean_captions(captions)

for img, caps in captions_cleaned.items():
    captions_cleaned[img] = ['startseq ' + cap + ' endseq' for cap in caps]

all_captions = []
for caps in captions_cleaned.values():
    all_captions.extend(caps)
# falttens all captions into one list
# convert words into integer IDs.
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1
print("Vocabulary size:", vocab_size)

max_length = max(len(c.split()) for c in all_captions)
print("Max caption length:", max_length)
#Returns the length of the longest caption (in words). Used to pad sequences.

def extract_features_batch(directory, batch_size=32, model_type='efficientnet'):

    if model_type == 'efficientnet': #load EfficientNetB0
        base_model = EfficientNetB7(weights='imagenet', include_top=False, pooling='avg')
    elif model_type == 'resnet':
        base_model = ResNet152(weights='imagenet', include_top=False, pooling='avg')
    else:
        base_model = VGG16(weights='imagenet')
        base_model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)

    features = {}
    image_paths = [f for f in os.listdir(directory) if f.lower().endswith('.jpg')]

    for i in tqdm(range(0, len(image_paths), batch_size)):
        batch_paths = image_paths[i:i+batch_size]
        batch_images = []

        for img_name in batch_paths:
            img_path = os.path.join(directory, img_name)
            img = preprocess_image(img_path)
            if img is not None:
                batch_images.append(img)

        if batch_images:
            batch_images = np.vstack(batch_images)
            batch_features = base_model.predict(batch_images, verbose=0)

            for j, img_name in enumerate(batch_paths):
                if j < len(batch_features):
                    features[img_name] = batch_features[j].flatten()

    print(f"\nTotal extracted features ({model_type.upper()}):", len(features))
    return features

features = extract_features_batch("Flicker8k_Dataset", model_type='efficientnet')

class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, descriptions, photos, tokenizer, max_length, vocab_size,
                 batch_size=32, shuffle=True, feature_dim=2560):
                  # Constructor that initializes key components.
                  #Saves all the parameters as instance attributes.
        self.descriptions = descriptions
        self.photos = photos
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.vocab_size = vocab_size
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.feature_dim = feature_dim

        self.image_ids = [img_id for img_id in descriptions.keys()
                         if img_id in photos and photos[img_id] is not None]
                         #Filters image IDs to only include those that exist in both descriptions and photos,
                         # and ensures the photo feature is not None.
        self.on_epoch_end()
        #Calls a method to shuffle image IDs if shuffle=True

    def __len__(self): #no of batches
        return int(np.ceil(len(self.image_ids) / self.batch_size)) #no of batches

    def __getitem__(self, index): # gets a slice of data , picks image IDS
        batch_ids = self.image_ids[index*self.batch_size:(index+1)*self.batch_size]
        X1, X2, y = self.__data_generation(batch_ids)

        #If the batch is empty, it just returns empty arrays to avoid errors.


        if len(X1) == 0:
            X1 = np.zeros((0, self.feature_dim))
            X2 = np.zeros((0, self.max_length))
            y = np.zeros((0, self.vocab_size))

        return {'input_1': tf.convert_to_tensor(X1, dtype=tf.float32),
                'input_2': tf.convert_to_tensor(X2, dtype=tf.int32)}, \
               tf.convert_to_tensor(y, dtype=tf.float32)


    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.image_ids)
#Helps prevent overfitting by feeding data in a different order every tim


    def __data_generation(self, batch_ids):
        X1, X2, y = [], [], []

        #These lists will store the batch data.

        for image_id in batch_ids:
            photo = self.photos.get(image_id)  #Get the image features for this ID.

            if photo is None:
                continue

            if len(photo) != self.feature_dim:
                continue

            desc_list = self.descriptions.get(image_id, [])
            #Turn each caption into a list of word indices.
            for desc in desc_list:
                seq = self.tokenizer.texts_to_sequences([desc])[0]
                # Get the image features for this ID.

                for i in range(1, len(seq)):
                    in_seq, out_seq = seq[:i], seq[i]
                    in_seq = pad_sequences([in_seq], maxlen=self.max_length, padding='post')[0]
                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]
                    X1.append(photo)
                    X2.append(in_seq)
                    y.append(out_seq)

        if len(X1) > 0:
            return np.array(X1), np.array(X2), np.array(y)
        return np.zeros((0, self.feature_dim)), np.zeros((0, self.max_length)), np.zeros((0, self.vocab_size))

    def get_sample_weights(self): #gives more weight to rare words
        """Optional: Compute sample weights to handle class imbalance"""
        word_counts = np.zeros(self.vocab_size)
        for desc_list in self.descriptions.values():
            for desc in desc_list:
                seq = self.tokenizer.texts_to_sequences([desc])[0]
                for word_idx in seq:
                    word_counts[word_idx] += 1

        weights = 1. / (word_counts + 1e-6)
        return weights / weights.mean()

def build_model(vocab_size, max_length, feature_size=2560):
    input1 = Input(shape=(feature_size,), name='input_1')

    fe1 = Dense(1024, activation='relu', kernel_regularizer=l2(1e-4))(input1)
    fe1 = LayerNormalization()(fe1)
    fe1 = Dropout(0.4)(fe1)

    fe2 = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(fe1)
    fe2 = BatchNormalization()(fe2)
    image_features = Dropout(0.3)(fe2)

    input2 = Input(shape=(max_length,), name='input_2')

    se1 = Embedding(vocab_size, 512, mask_zero=True)(input2)
    se1 = Dropout(0.3)(se1)

    gru1 = GRU(512, return_sequences=False, implementation=0)(se1)
    gru1 = LayerNormalization()(gru1)
    gru1 = Dropout(0.3)(gru1)

    combined = Concatenate()([image_features, gru1])

    decoder1 = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(combined)
    decoder1 = BatchNormalization()(decoder1)
    decoder1 = Dropout(0.4)(decoder1)

    decoder2 = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(decoder1)
    decoder2 = LayerNormalization()(decoder2)

    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[input1, input2], outputs=outputs)

    optimizer = Adam(
        learning_rate=0.0005,
        clipnorm=1.0
    )

    model.compile(
        loss='categorical_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy']
    )

    return model

image_ids = list(captions_cleaned.keys())
random.shuffle(image_ids)

split_index = int(0.8 * len(image_ids))
train_image_ids = image_ids[:split_index]
test_image_ids = image_ids[split_index:]

train_descriptions = {img: captions_cleaned[img] for img in train_image_ids}
test_descriptions = {img: captions_cleaned[img] for img in test_image_ids}

batch_size = 32

train_generator = DataGenerator(train_descriptions, features, tokenizer, max_length, vocab_size, batch_size)
test_generator = DataGenerator(test_descriptions, features, tokenizer, max_length, vocab_size, batch_size)

model = build_model(vocab_size, max_length)
model.summary()

checkpoint = ModelCheckpoint('best_model.keras',
                           monitor='val_loss',
                           save_best_only=True,
                           mode='min')

early_stopping = EarlyStopping(monitor='val_loss',
                             patience=3,
                             restore_best_weights=True)

history = model.fit(
    train_generator,
    epochs=20,
    validation_data=test_generator,
    callbacks=[checkpoint, early_stopping],
    verbose=1
)

def generate_caption(model, image_path, tokenizer, max_length, model_type='efficientnet'):
    photo = preprocess_image(image_path, model_type=model_type)
    if photo is None:
        print("Image preprocessing failed.")
        return None

    if model_type == 'efficientnet':
        feature_model = EfficientNetB7(weights='imagenet', include_top=False, pooling='avg')
    elif model_type == 'resnet':
        feature_model = ResNet152(weights='imagenet', include_top=False, pooling='avg')
    else:
        feature_model = VGG16(weights='imagenet')
        feature_model = Model(inputs=feature_model.inputs, outputs=feature_model.layers[-2].output)

    feature = feature_model.predict(photo, verbose=0)

    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')

        yhat = model.predict([feature, sequence], verbose=0)
        yhat_idx = np.argmax(yhat)
        word = tokenizer.index_word.get(yhat_idx, None)

        if word is None or word == 'endseq':
            break

        in_text += ' ' + word

    return in_text.replace('startseq', '').replace('endseq', '').strip()

def display_image_with_caption(image_path, caption, target_size=(600, 600)):
    img = load_img(image_path, target_size=target_size)
    plt.figure(figsize=(10, 8))
    plt.imshow(img)
    plt.axis('off')

    wrapped_caption = '\n'.join([caption[i:i+80] for i in range(0, len(caption), 80)])
    plt.title(wrapped_caption, fontsize=12, pad=20)
    plt.tight_layout()
    plt.show()


image_paths = [
    '/kaggle/input/pic-dla/gp8_15e29595.jpg',
    '/kaggle/input/meralshma3a/WhatsApp Image 2025-04-30 at 04.02.12 (1).jpeg',
    '/kaggle/input/people/WhatsApp Image 2025-04-30 at 05.04.29.jpeg',
    '/kaggle/input/dancing/WhatsApp Image 2025-04-30 at 04.57.33.jpeg',
    '/kaggle/input/pic-dla/babys.jpg',
    '/kaggle/input/pic-dla/WhatsApp Image 2025-04-29 at 23.01.31_b2b63591.jpg'
]

trained_model = load_model('best_model.keras')

for image_path in image_paths:
    try:
        caption = generate_caption(
            trained_model,
            image_path,
            tokenizer,
            max_length,
            model_type='efficientnet'
        )
        print(f"Image: {os.path.basename(image_path)}")
        print(f"Caption: {caption}\n")
        display_image_with_caption(image_path, caption)
    except Exception as e:
        print(f"Error processing {image_path}: {str(e)}")

